{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "from taco_dataset import TACODataset\n",
    "from model import get_model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data loader\n",
    "    Args:\n",
    "        model: the model to evaluate\n",
    "        data_loader: the data loader to evaluate the model on\n",
    "        device: the device to evaluate the model on (cuda / cpu)\n",
    "    Returns:\n",
    "        metric: the metrics to evaluate the model (mAP, precision, recall, etc.)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # predict and metric update\n",
    "            predictions = model(images)\n",
    "            metric.update(predictions, targets)\n",
    "\n",
    "    result = metric.compute()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, idx, device, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualize the predictions of the model on the given dataset\n",
    "    Args:\n",
    "        model: the model to evaluate\n",
    "        dataset: the dataset to evaluate the model on\n",
    "        idx: the index of the image to visualize\n",
    "        device: the device to evaluate the model on (cuda / cpu)\n",
    "        confidence_threshold: the confidence threshold to visualize the predictions\n",
    "    \"\"\"\n",
    "    image, target = dataset[idx]\n",
    "\n",
    "    # predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model([image.to(device)])\n",
    "        prediction = prediction[0]\n",
    "\n",
    "    # original image\n",
    "    image = torch.tensor(image)\n",
    "    image = torchvision.transforms.functional.normalize(\n",
    "        image,\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # visualize the results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image)\n",
    "    # plt.title(\"Ground Truth\")\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    # visualize the ground truth boxes (green)\n",
    "    for box, label in zip(target['boxes'], target['labels']):\n",
    "        box = box.cpu().numpy()\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]),\n",
    "            box[2] - box[0],\n",
    "            box[3] - box[1],\n",
    "            linewidth=2,\n",
    "            edgecolor='g',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.text(\n",
    "            box[0], box[1]-5,\n",
    "            f'GT: {dataset.cat_ids[label.item()]}',\n",
    "            color='white', bbox=dict(facecolor='green', alpha=0.5)\n",
    "        )\n",
    "    \n",
    "    # 예측 박스 (빨간색)\n",
    "    for box, label, score in zip(prediction['boxes'], prediction['labels'], prediction['scores']):\n",
    "        if score > confidence_threshold:\n",
    "            box = box.cpu().numpy()\n",
    "            rect = patches.Rectangle(\n",
    "                (box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
    "                linewidth=2, edgecolor='r', facecolor='none'\n",
    "            )\n",
    "            plt.gca().add_patch(rect)\n",
    "            plt.text(\n",
    "                box[0], box[1]-20,\n",
    "                f'Pred: {dataset.cat_ids[label.item()]}: {score:.2f}',\n",
    "                color='white', bbox=dict(facecolor='red', alpha=0.5)\n",
    "            )\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'annotations_0_test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m checkpoints_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# load test dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTACODataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mannotations_0_test.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mx)))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 모든 체크포인트 파일 찾기\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/EarthMera_Research/Trash_detection/taco_dataset.py:24\u001b[0m, in \u001b[0;36mTACODataset.__init__\u001b[0;34m(self, root_dir, annotation_file, transform)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform \u001b[38;5;129;01mor\u001b[39;00m T\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     18\u001b[0m     T\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     19\u001b[0m     T\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],\n\u001b[1;32m     20\u001b[0m                std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     21\u001b[0m ])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Load annotations\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Create category mapping\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'annotations_0_test.json'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir = './data'\n",
    "checkpoints_dir = './checkpoints'\n",
    "\n",
    "# load test dataset\n",
    "test_dataset = TACODataset(root_dir=data_dir, annotation_file='annotations_0_test.json')\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# 모든 체크포인트 파일 찾기\n",
    "checkpoint_files = [f for f in os.listdir(checkpoints_dir) if f.startswith('model_epoch_') and f.endswith('.pth')]\n",
    "checkpoint_files.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))  # epoch 번호순으로 정렬\n",
    "\n",
    "# 결과를 저장할 딕셔너리\n",
    "results_dict = {\n",
    "    'epoch': [],\n",
    "    'mAP': [],\n",
    "    'mAP_50': [],\n",
    "    'mAP_75': []\n",
    "}\n",
    "\n",
    "# 각 체크포인트별로 평가 수행\n",
    "for checkpoint_file in checkpoint_files:\n",
    "    epoch = int(checkpoint_file.split('_')[2].split('.')[0])\n",
    "    checkpoint_path = os.path.join(checkpoints_dir, checkpoint_file)\n",
    "    print(f\"\\nEvaluating checkpoint: {checkpoint_file}\")\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = get_model(test_dataset.num_classes)\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    \n",
    "    # 평가 수행\n",
    "    results = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # 결과 저장\n",
    "    results_dict['epoch'].append(epoch)\n",
    "    results_dict['mAP'].append(results['map'].item())\n",
    "    results_dict['mAP_50'].append(results['map_50'].item())\n",
    "    results_dict['mAP_75'].append(results['map_75'].item())\n",
    "    \n",
    "    print(f\"Epoch {epoch}:\")\n",
    "    print(f\"mAP @ 0.5:0.95: {results['map'].item():.4f}\")\n",
    "    print(f\"mAP @ 0.5: {results['map_50'].item():.4f}\")\n",
    "    print(f\"mAP @ 0.75: {results['map_75'].item():.4f}\")\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "print(\"\\nAll Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# 결과 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['epoch'], results_df['mAP'], label='mAP@0.5:0.95', marker='o')\n",
    "plt.plot(results_df['epoch'], results_df['mAP_50'], label='mAP@0.5', marker='o')\n",
    "plt.plot(results_df['epoch'], results_df['mAP_75'], label='mAP@0.75', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('mAP')\n",
    "plt.title('Model Performance across Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 최고 성능 체크포인트 찾기\n",
    "best_map = results_df.loc[results_df['mAP'].idxmax()]\n",
    "print(\"\\nBest Checkpoint:\")\n",
    "print(f\"Epoch: {best_map['epoch']}\")\n",
    "print(f\"mAP @ 0.5:0.95: {best_map['mAP']:.4f}\")\n",
    "print(f\"mAP @ 0.5: {best_map['mAP_50']:.4f}\")\n",
    "print(f\"mAP @ 0.75: {best_map['mAP_75']:.4f}\")\n",
    "\n",
    "# 최고 성능 모델로 예시 이미지 시각화\n",
    "best_checkpoint_path = os.path.join(checkpoints_dir, f\"model_epoch_{int(best_map['epoch'])}.pth\")\n",
    "best_model = get_model(test_dataset.num_classes)\n",
    "best_model.load_state_dict(torch.load(best_checkpoint_path)['model_state_dict'])\n",
    "best_model.to(device)\n",
    "\n",
    "print(\"\\nVisualizing predictions with best model:\")\n",
    "num_visualizations = 5\n",
    "for i in range(num_visualizations):\n",
    "    print(f\"\\nExample {i+1}/{num_visualizations}\")\n",
    "    visualize_predictions(best_model, test_dataset, i, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthmera_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
